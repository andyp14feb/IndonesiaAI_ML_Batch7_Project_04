{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98b45434",
   "metadata": {},
   "source": [
    "Data taken from : \n",
    "\n",
    "- https://www.kaggle.com/datasets/gauravduttakiit/smoker-status-prediction-using-biosignals/data (not used)\n",
    "\n",
    "- https://www.kaggle.com/competitions/playground-series-s3e24/overview\n",
    "\n",
    "About Dataset\n",
    "Smoking has been proven to negatively affect health in a multitude of ways.Smoking has been found to harm nearly every organ of the body, cause many diseases, as well as reducing the life expectancy of smokers in general. As of 2018, smoking has been considered the leading cause of preventable morbidity and mortality in the world, continuing to plague the world’s overall health.\n",
    "\n",
    "According to a World Health Organization report, the number of deaths caused by smoking will reach 10 million by 2030.\n",
    "\n",
    "Evidence-based treatment for assistance in smoking cessation had been proposed and promoted. however, only less than one third of the participants could achieve the goal of abstinence. Many physicians found counseling for smoking cessation ineffective and time-consuming, and did not routinely do so in daily practice. To overcome this problem, several factors had been proposed to identify smokers who had a better chance of quitting, including the level of nicotine dependence, exhaled carbon monoxide (CO) concentration, cigarette amount per day, the age at smoking initiation, previous quit attempts, marital status, emotional distress, temperament and impulsivity scores, and the motivation to stop smoking. However, individual use of these factors for prediction could lead to conflicting results that were not straightforward enough for the physicians and patients to interpret and apply. Providing a prediction model might be a favorable way to understand the chance of quitting smoking for each individual smoker. Health outcome prediction models had been developed using methods of machine learning over recent years.\n",
    "\n",
    "A group of scientists are working on predictive models with smoking status as the prediction target.Your task is to help them create a machine learning model to identify the smoking status of an individual using bio-signals\n",
    "\n",
    "Dataset Description\n",
    "\n",
    "| Feature | Description |\n",
    "| --- | --- |\n",
    "| age | 5-year intervals |\n",
    "| height (cm) | Height in centimeters |\n",
    "| weight (kg) | Weight in kilograms |\n",
    "| waist (cm) | Waist circumference |\n",
    "| eyesight (left) | Vision (left eye) |\n",
    "| eyesight (right) | Vision (right eye) |\n",
    "| hearing (left) | Hearing (left ear) |\n",
    "| hearing (right) | Hearing (right ear) |\n",
    "| systolic | Systolic blood pressure |\n",
    "| relaxation | Diastolic (relaxation) blood pressure |\n",
    "| fasting blood sugar | Fasting blood sugar |\n",
    "| Cholesterol | Total cholesterol |\n",
    "| triglyceride | Triglyceride |\n",
    "| HDL | High-density lipoprotein cholesterol |\n",
    "| LDL | Low-density lipoprotein cholesterol |\n",
    "| hemoglobin | Hemoglobin level |\n",
    "| Urine protein | Urine protein level |\n",
    "| serum creatinine | Serum creatinine |\n",
    "| AST | Glutamic oxaloacetic transaminase |\n",
    "| ALT | Glutamic pyruvic transaminase |\n",
    "| Gtp | γ-GTP (Gamma-glutamyl transferase) |\n",
    "| dental caries | Dental caries (yes/no) |\n",
    "| smoking | Smoking status (target variable) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd1d422",
   "metadata": {},
   "source": [
    "# IMPORTS and LOAD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49bbfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from mini_eda_module import mini_eda # https://github.com/andyp14feb/miniEDAtoPDF\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "df = pd.read_csv('./data/train.csv')\n",
    "df = df.drop('id', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f04a2c",
   "metadata": {},
   "source": [
    "# Function Colloection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa41e7f",
   "metadata": {},
   "source": [
    "## capping_features_based_on_threshold(df, thresholds_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8473b92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capping_features_based_on_threshold(df, thresholds_df):\n",
    "    for _, row in thresholds_df.iterrows():\n",
    "        col = row['feature']\n",
    "        min_val = row['min']\n",
    "        max_val = row['max']\n",
    "        \n",
    "        flag_col = f\"impossibleValue_{col}\"\n",
    "        df[flag_col] = ''\n",
    "        \n",
    "        below_flag = f\"#{col}_belowMinThreshold; \"\n",
    "        above_flag = f\"#{col}_aboveMaxThreshold; \"\n",
    "        \n",
    "        df.loc[df[col] < min_val, flag_col] += below_flag\n",
    "        df.loc[df[col] > max_val, flag_col] += above_flag\n",
    "        \n",
    "        df[col] = df[col].clip(lower=min_val, upper=max_val)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f782da4e",
   "metadata": {},
   "source": [
    "## calculate_IQR(df, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a30903b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_IQR(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    \n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Identify outliers (data points outside the lower and upper bounds)\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    num_outliers = outliers.shape[0]\n",
    "    percent_outliers = (num_outliers / len(df)) * 100\n",
    "    above_upper = df[df[column] > upper_bound].shape[0]\n",
    "    below_lower = df[df[column] < lower_bound].shape[0]\n",
    "    \n",
    "    return lower_bound, upper_bound, num_outliers, percent_outliers, above_upper, below_lower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f93b7a9",
   "metadata": {},
   "source": [
    "## clip_values_based_on_IQR(df, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9f47c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_values_based_on_IQR(df, column):\n",
    "    # Calculate IQR, lower and upper bounds, and outliers\n",
    "    lower_bound, upper_bound, _, _, _, _ = calculate_IQR(df, column)\n",
    "    \n",
    "    # Clip values outside the lower and upper bounds\n",
    "    df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504de9bc",
   "metadata": {},
   "source": [
    "## show_corrMap(pandas_df, savedImageName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc3149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_corrMap(pandas_df, savedImageName):\n",
    "    df_corr = pandas_df.corr(numeric_only=True)\n",
    "    # --- create upper-triangle mask to avoid duplicates (optional) ---\n",
    "    mask = np.triu(np.ones_like(df_corr, dtype=bool))\n",
    "\n",
    "    plt.figure(figsize=(11, 9))\n",
    "    sns.heatmap(\n",
    "        df_corr,\n",
    "        # mask=mask,              # comment this line if you want full matrix\n",
    "        cmap='coolwarm',\n",
    "        center=0,\n",
    "        vmin=-1, vmax=1,\n",
    "        annot=True,             # <-- numbers on cells\n",
    "        fmt=\".2f\",              # 2-decimal format\n",
    "        annot_kws={\"size\": 8},  # font size for numbers\n",
    "        linewidths=.5,\n",
    "        square=True,\n",
    "        cbar_kws={\"shrink\": .8}\n",
    "    )\n",
    "    plt.title('Feature Correlation Matrix (annotated)', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(savedImageName, dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2dee37",
   "metadata": {},
   "source": [
    "## add_combined_features(dfnya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2360431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_combined_features(dfnya):\n",
    "    #kurus/gemuk (ukuran kegemukan)\n",
    "    dfnya['new_bmi'] = (dfnya['weight_kg'] / (dfnya['height_cm'] ** 2))*100\n",
    "\n",
    "    #lingkarpinggang (ukuran perut)\n",
    "    dfnya['new_waist_height_ratio']=dfnya['waist_cm'] / dfnya['height_cm']\n",
    "\n",
    "    #kekuatan pompa jantung\n",
    "    dfnya['new_pulse_preasure'] = dfnya['systolic']-dfnya['relaxation']\n",
    "\n",
    "    #penglihatan_rata-rata\n",
    "    dfnya['new_vision'] = (dfnya['eyesight_right']+dfnya['eyesight_left'])/2\n",
    "\n",
    "    #hearing_rata-rata\n",
    "    dfnya['new_hearing'] = (dfnya['hearing_right']+dfnya['hearing_left'])/2\n",
    "\n",
    "    #good cholesterol ratio\n",
    "    dfnya['new_good_chol_ratio'] = (dfnya['hdl'] / dfnya['cholesterol'])*100\n",
    "\n",
    "    #bad-good cholesterol ratio\n",
    "    dfnya['new_bad_good_chol_ratio'] = (dfnya['ldl'] / dfnya['hdl'])*100\n",
    "\n",
    "    #liverEnzimeRatio\n",
    "    dfnya['new_liverEnzimeRatio'] = (dfnya['ast'] / dfnya['alt'])*100\n",
    "\n",
    "    return dfnya\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272665ba",
   "metadata": {},
   "source": [
    "## calculate_vif(df, target_column='smoking', vif_threshold=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbeaa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vif(df, target_column='smoking', vif_threshold=5.0):\n",
    "    # Drop target\n",
    "    X = df.drop(columns=[target_column])\n",
    "    # Drop non-numeric columns if any (e.g., ID or categorical)\n",
    "    X = X.select_dtypes(include='number')\n",
    "    \n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Compute VIF\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data['feature'] = X.columns\n",
    "    vif_data['VIF'] = [variance_inflation_factor(X_scaled, i) for i in range(X_scaled.shape[1])]\n",
    "    \n",
    "    # Sort by VIF descending\n",
    "    vif_data = vif_data.sort_values(by='VIF', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    print('-------------------------------------------------------------')\n",
    "    print(vif_data)\n",
    "    print('-------------------------------------------------------------')\n",
    "    print(\"High VIF features (possible multicollinearity):\")\n",
    "    print(vif_data[vif_data['VIF'] > vif_threshold])\n",
    "    \n",
    "    return vif_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd94a7c2",
   "metadata": {},
   "source": [
    "## remove_highly_correlated_features(df, target_column, threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5b4d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_highly_correlated_features(df, target_column, threshold=0.7):\n",
    "    # Step 1: Calculate the correlation matrix\n",
    "    correlation_matrix = df.corr()\n",
    "    # print(f'correlation_matrix: {correlation_matrix}')\n",
    "    \n",
    "    # Step 2: Find pairs of features with correlation > threshold (absolute value)\n",
    "    high_corr_pairs = correlation_matrix.unstack().sort_values(ascending=False)\n",
    "    # print(f'Highly correlated pairs 1 :\\n{high_corr_pairs}\\n')\n",
    "\n",
    "    high_corr_pairs = high_corr_pairs[high_corr_pairs.abs() > threshold]\n",
    "    # print(f'Highly correlated pairs 2 :\\n{high_corr_pairs}\\n')\n",
    "    \n",
    "    # Step 3: Remove diagonal (self-correlations)\n",
    "    high_corr_pairs = high_corr_pairs[high_corr_pairs < 1]\n",
    "    print(f'Highly correlated pairs 3 :\\n{high_corr_pairs}\\n')\n",
    "    \n",
    "    # Step 4: Find feature importance with respect to the target variable\n",
    "    target_corr = correlation_matrix[target_column].sort_values(ascending=False)\n",
    "    print(f'Target variable correlation:\\n{target_corr}\\n')\n",
    "    \n",
    "    # Step 5: Iterate through the highly correlated pairs and remove less important feature\n",
    "    features_to_remove = []\n",
    "    for pair in high_corr_pairs.index:\n",
    "        feature1, feature2 = pair\n",
    "        # Remove the feature with lower correlation to the target\n",
    "        if abs(correlation_matrix[feature1][target_column]) < abs(correlation_matrix[feature2][target_column]):\n",
    "            features_to_remove.append(feature1)\n",
    "        else:\n",
    "            features_to_remove.append(feature2)\n",
    "    \n",
    "    # Remove the features that are less important (with lower correlation to the target)\n",
    "    df.drop(columns=features_to_remove, inplace=True)\n",
    "    \n",
    "    return df, features_to_remove\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd17764",
   "metadata": {},
   "source": [
    "## splitting_balancing_scaling(the_df, target_column):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28f384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting_balancing_scaling(the_df, target_column, df_name=''):\n",
    "    # Ensure that index is consistent\n",
    "    the_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # SPLITTING\n",
    "    print(f'Splitting the {df_name}')\n",
    "    print('-----------------------------------')\n",
    "\n",
    "    X = the_df.drop(columns=[target_column])\n",
    "    y = the_df[target_column]\n",
    "\n",
    "    # Perform train-test split, with stratification for target column\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    print(\"Shape of the_df:\", the_df.shape)\n",
    "    print(\"Shape of X:\", X.shape)\n",
    "    print(\"Shape of y:\", y.shape)\n",
    "    print(\"Shape of X_train:\", X_train.shape)\n",
    "    print(\"Shape of X_test:\", X_test.shape)\n",
    "    print(\"Shape of y_train:\", y_train.shape)\n",
    "    print(\"Shape of y_test:\", y_test.shape)\n",
    "    print(\"===================================\")\n",
    "\n",
    "    # BALANCING\n",
    "    print(f'Balancing training data')\n",
    "    print('-----------------------------------')\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_train_balanced, y_train_balanced = sm.fit_resample(X_train, y_train)\n",
    "    print(\"Original dataset shape:\", X_train.shape, y_train.shape)\n",
    "    print(\"Balanced dataset shape:\", X_train_balanced.shape, y_train_balanced.shape)\n",
    "    print(f\"Original y_train:\\n\", y_train.value_counts(normalize=True))\n",
    "    print(f\"Balanced y_train:\\n\", y_train_balanced.value_counts(normalize=True))\n",
    "    print(\"===================================\")\n",
    "\n",
    "    # SCALING\n",
    "    print(f'Scaling X_train and X_test data')\n",
    "    print('-----------------------------------')\n",
    "    scaler = StandardScaler()\n",
    "    X_train_balanced_scaled = scaler.fit_transform(X_train_balanced)  # Fit and transform for training data\n",
    "    X_test_scaled = scaler.transform(X_test)  # Only transform for test data\n",
    "\n",
    "    return X_train_balanced_scaled, y_train_balanced, X_test_scaled, y_test, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d4b0a6",
   "metadata": {},
   "source": [
    "## drop_column_if_exists(df, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bb0c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_column_if_exists(df, column_name):\n",
    "    if column_name in df.columns:\n",
    "        df.drop([column_name], axis=1, inplace=True)\n",
    "        print(f\"Column '{column_name}' dropped successfully.\")\n",
    "    else:\n",
    "        print(f\"Column '{column_name}' does not exist.\")    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f426f62f",
   "metadata": {},
   "source": [
    "## drop_new_feature_yg_lemah_atau_feature_originalnya(df,listnya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6733d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_new_feature_yg_lemah_atau_feature_originalnya(df,listnya):\n",
    "    # fungsi ini membandingkan 2 feature pembentuk feature baru \n",
    "    # listnya adalah list dengan urutan FeatureLama1, FeatureLama2, FeatureBentukanBaru\n",
    "    # jika feature baru justru lebih lemah maka feature baru dibuang saja (gak jadi dipakai)\n",
    "    # jika feature baru paling tinggi , maka feature lama keduanya dibuang\n",
    "    # jika salah satu feature saja yang lebih lemah dari feature baru, maka feature lemah itu yang dibuang\n",
    "\n",
    "    corrnya=df.corr(numeric_only=True).loc['smoking', listnya]\n",
    "    corrnya = corrnya.abs()\n",
    "    print(f'{corrnya}\\nthe lowest value => {corrnya.idxmin()} = {corrnya.min()}')\n",
    "    if corrnya.idxmax() == listnya[2]:\n",
    "        df=drop_column_if_exists(df,listnya[0])\n",
    "        df=drop_column_if_exists(df,listnya[1])\n",
    "    else:\n",
    "        df=drop_column_if_exists(df,corrnya.idxmin())\n",
    "\n",
    "    print('\\n\\n-------------------------------\\n\\n')    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c367b00",
   "metadata": {},
   "source": [
    "## save_model(model, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2115567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "    print(f\"Model saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fec755",
   "metadata": {},
   "source": [
    "## load_model(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ccf322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "    print(f\"Model loaded from {filename}\")\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7f0761",
   "metadata": {},
   "source": [
    "## load_model_and_predict(model_file_path,model_file_name, X_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b8ff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_predict(model_file_path,model_file_name, X_test_scaled,y_test):\n",
    "    try:\n",
    "        model = load_model(f'{model_file_path}{model_file_name}')\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        print(f\"== {model_file_name} ==\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: There was a mismatch in the data or model. Error details: {ve}\")\n",
    "    except AttributeError as ae:\n",
    "        print(f\"AttributeError: The model might be incompatible with the provided data. Error details: {ae}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error during prediction: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecb9c4d",
   "metadata": {},
   "source": [
    "## hyper_parameter_tuning_LogisticReg_gridSearch_recall(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a613b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_parameter_tuning_LogisticReg_gridSearch_recall(X_train, y_train):\n",
    "\n",
    "    # Parameter grid\n",
    "    logreg_param_grid = {\n",
    "        'clf__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'clf__penalty': ['l1', 'l2'],\n",
    "        'clf__solver': ['liblinear'],\n",
    "        'clf__fit_intercept': [True, False],\n",
    "        # 'clf__class_weight': [None, 'balanced', {0: 1, 1: 2}, {0: 1, 1: 3}]\n",
    "        'clf__class_weight': [None, 'balanced']\n",
    "    }\n",
    "\n",
    "    # Pipeline (with class_weight)\n",
    "    logreg_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
    "    ])\n",
    "\n",
    "    # Custom scorer\n",
    "    recall_scorer = make_scorer(recall_score, pos_label=1)\n",
    "\n",
    "    # GridSearchCV\n",
    "    logreg_grid = GridSearchCV(logreg_pipeline,\n",
    "                            param_grid=logreg_param_grid,\n",
    "                            scoring=recall_scorer,\n",
    "                            cv=5,\n",
    "                            verbose=1,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "    logreg_grid.fit(X_train, y_train)\n",
    "\n",
    "    best_logreg_model = logreg_grid.best_estimator_\n",
    "    best_logreg_param = logreg_grid.best_params_\n",
    "    print(\"Logistic Regression ; Grid Search \")\n",
    "    \n",
    "    print(\"Best Parameters:\")\n",
    "    for key, value in  logreg_grid.best_params_.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    print(\"--------------------------------------------\")\n",
    "    return best_logreg_model ,best_logreg_param"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cb7294",
   "metadata": {},
   "source": [
    "## hyper_parameter_tuning_LogisticReg_gridSearch_accuracy(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc9dba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_parameter_tuning_LogisticReg_gridSearch_accuracy(X_train, y_train):\n",
    "    # Parameter grid\n",
    "    logreg_param_grid = {\n",
    "        'clf__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'clf__penalty': ['l1', 'l2'],\n",
    "        'clf__solver': ['liblinear'],\n",
    "        'clf__fit_intercept': [True, False],\n",
    "        # 'clf__class_weight': [None, 'balanced', {0: 1, 1: 2}, {0: 1, 1: 3}] kalau data sudah balanced tidak butuh ini\n",
    "        'clf__class_weight': [None, 'balanced']\n",
    "        \n",
    "    }\n",
    "\n",
    "    # Pipeline\n",
    "    logreg_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(max_iter=1000))\n",
    "    ])\n",
    "\n",
    "    # Use accuracy as scoring metric\n",
    "    accuracy_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "    # GridSearchCV\n",
    "    logreg_grid = GridSearchCV(\n",
    "        estimator=logreg_pipeline,\n",
    "        param_grid=logreg_param_grid,\n",
    "        scoring=accuracy_scorer,\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    logreg_grid.fit(X_train, y_train)\n",
    "\n",
    "    best_logreg_model = logreg_grid.best_estimator_\n",
    "    best_logreg_param = logreg_grid.best_params_\n",
    "\n",
    "    print(\"Logistic Regression ; Grid Search (Accuracy Focused)\")\n",
    "    print(\"Best Parameters:\")\n",
    "    for key, value in best_logreg_param.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(\"--------------------------------------------\")\n",
    "\n",
    "    return best_logreg_model, best_logreg_param\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c5bd7d",
   "metadata": {},
   "source": [
    "## hyper_parameter_tuning_LogisticReg_gridSearch_AUC(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54c3209",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hyper_parameter_tuning_LogisticReg_gridSearch_AUC(X_train, y_train):\n",
    "    logreg_param_grid = {\n",
    "        'clf__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'clf__penalty': ['l1', 'l2'],\n",
    "        'clf__solver': ['liblinear'],  # 'liblinear' supports both l1 and l2\n",
    "        'clf__fit_intercept': [True, False],\n",
    "        'clf__class_weight': [None, 'balanced']\n",
    "    }\n",
    "\n",
    "    logreg_pipeline = Pipeline([\n",
    "        # ('scaler', StandardScaler()),  # ⛔ Commented out for flexibility, enable if needed\n",
    "        ('clf', LogisticRegression(max_iter=1000))\n",
    "    ])\n",
    "\n",
    "    logreg_grid = GridSearchCV(\n",
    "        estimator=logreg_pipeline,\n",
    "        param_grid=logreg_param_grid,\n",
    "        scoring='roc_auc',  # ✅ Fokus ke AUC\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    logreg_grid.fit(X_train, y_train)\n",
    "\n",
    "    best_logreg_model = logreg_grid.best_estimator_\n",
    "    best_logreg_param = logreg_grid.best_params_\n",
    "\n",
    "    print(\"Logistic Regression ; Grid Search (AUC Focused)\")\n",
    "    print(\"Best Parameters:\")\n",
    "    for key, value in best_logreg_param.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(\"--------------------------------------------\")\n",
    "\n",
    "    return best_logreg_model, best_logreg_param\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b12c05",
   "metadata": {},
   "source": [
    "## hyper_parameter_tuning_randomForest_randomSearch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a22955",
   "metadata": {},
   "source": [
    "| Parameter           | Description                                        |\n",
    "| ------------------- | -------------------------------------------------- |\n",
    "| `n_estimators`      | Number of trees in the forest                      |\n",
    "| `max_depth`         | Maximum depth of each tree                         |\n",
    "| `min_samples_split` | Minimum samples required to split a node           |\n",
    "| `min_samples_leaf`  | Minimum samples at a leaf node                     |\n",
    "| `max_features`      | Number of features to consider at each split       |\n",
    "| `class_weight`      | Helps to boost recall for class 1 (same as before) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb3aee9",
   "metadata": {},
   "source": [
    "## hyper_parameter_tuning_randomForest_randomSearch_recall(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253d8883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_parameter_tuning_randomForest_randomSearch_recall(X_train, y_train):\n",
    "\n",
    "    # Parameter search space\n",
    "    rf_param_grid = {\n",
    "        'clf__n_estimators': [100, 200, 300, 400],\n",
    "        'clf__max_depth': [None, 10, 20, 30, 40],\n",
    "        'clf__min_samples_split': [2, 5, 10],\n",
    "        'clf__min_samples_leaf': [1, 2, 4],\n",
    "        'clf__max_features': ['sqrt', 'log2'],\n",
    "        # 'clf__class_weight': [None, 'balanced', {0: 1, 1: 2}, {0: 1, 1: 3}]\n",
    "        'clf__class_weight': [None, 'balanced']\n",
    "    }\n",
    "\n",
    "    # Create pipeline\n",
    "    rf_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "\n",
    "    # Focus on recall for class 1\n",
    "    recall_scorer = make_scorer(recall_score, pos_label=1)\n",
    "\n",
    "    # RandomizedSearchCV\n",
    "    rf_search = RandomizedSearchCV(rf_pipeline,\n",
    "                                    param_distributions=rf_param_grid,\n",
    "                                    n_iter=50,\n",
    "                                    scoring=recall_scorer,\n",
    "                                    cv=5,\n",
    "                                    verbose=1,\n",
    "                                    n_jobs=-1,\n",
    "                                    random_state=42)\n",
    "\n",
    "    rf_search.fit(X_train, y_train)\n",
    "\n",
    "    best_randomForest_model = rf_search.best_estimator_\n",
    "    best_randomForest_param = rf_search.best_params_\n",
    "    print(\"Random Forest ; Random Search \")\n",
    "    print(\"Best Parameters:\", best_randomForest_param)\n",
    "\n",
    "    print(\"Best Parameters:\")\n",
    "    for key, value in  best_randomForest_param.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(\"--------------------------------------------\")\n",
    "\n",
    "    return best_randomForest_model ,best_randomForest_param\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d7323b",
   "metadata": {},
   "source": [
    "## hyper_parameter_tuning_randomForest_randomSearch_accuracy(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5e430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_parameter_tuning_randomForest_randomSearch_accuracy(X_train, y_train):\n",
    "    # Parameter search space\n",
    "    rf_param_grid = {\n",
    "        'clf__n_estimators': [100, 200, 300, 400],\n",
    "        'clf__max_depth': [None, 10, 20, 30, 40],\n",
    "        'clf__min_samples_split': [2, 5, 10],\n",
    "        'clf__min_samples_leaf': [1, 2, 4],\n",
    "        'clf__max_features': ['sqrt', 'log2'],\n",
    "        # 'clf__class_weight': [None, 'balanced', {0: 1, 1: 2}, {0: 1, 1: 3}]\n",
    "        'clf__class_weight': [None, 'balanced']\n",
    "\n",
    "    }\n",
    "\n",
    "    # Create pipeline\n",
    "    rf_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "\n",
    "    # Accuracy scoring\n",
    "    accuracy_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "    # RandomizedSearchCV\n",
    "    rf_search = RandomizedSearchCV(\n",
    "        estimator=rf_pipeline,\n",
    "        param_distributions=rf_param_grid,\n",
    "        n_iter=50,\n",
    "        scoring=accuracy_scorer,\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    rf_search.fit(X_train, y_train)\n",
    "\n",
    "    best_randomForest_model = rf_search.best_estimator_\n",
    "    best_randomForest_param = rf_search.best_params_\n",
    "\n",
    "    print(\"Random Forest ; Random Search (Accuracy Focused)\")\n",
    "    print(\"Best Parameters:\")\n",
    "    for key, value in best_randomForest_param.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(\"--------------------------------------------\")\n",
    "\n",
    "    return best_randomForest_model, best_randomForest_param\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21554398",
   "metadata": {},
   "source": [
    "## hyper_parameter_tuning_randomForest_randomSearch_AUC(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c73182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_parameter_tuning_randomForest_randomSearch_AUC(X_train, y_train):\n",
    "    rf_param_grid = {\n",
    "        'clf__n_estimators': [100, 200, 300, 400],\n",
    "        'clf__max_depth': [None, 10, 20, 30, 40],\n",
    "        'clf__min_samples_split': [2, 5, 10],\n",
    "        'clf__min_samples_leaf': [1, 2, 4],\n",
    "        'clf__max_features': ['sqrt', 'log2'],\n",
    "        'clf__class_weight': [None, 'balanced']\n",
    "    }\n",
    "\n",
    "    rf_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "\n",
    "    rf_search = RandomizedSearchCV(\n",
    "        rf_pipeline,\n",
    "        param_distributions=rf_param_grid,\n",
    "        n_iter=50,\n",
    "        scoring='roc_auc',  # GANTI di sini\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    rf_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = rf_search.best_estimator_\n",
    "    best_params = rf_search.best_params_\n",
    "    print(\"Random Forest ; Random Search (AUC Optimized)\")\n",
    "    print(\"Best Parameters:\")\n",
    "    for k, v in best_params.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    print(\"--------------------------------------------\")\n",
    "\n",
    "    return best_model, best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50138a3e",
   "metadata": {},
   "source": [
    "## hyper_parameter_tuning_XGB_randomSearch_recall(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0297b0",
   "metadata": {},
   "source": [
    "| Parameter                 | Purpose                                                             |\n",
    "| ------------------------- | ------------------------------------------------------------------- |\n",
    "| `n_estimators`            | Number of boosting rounds (trees)                                   |\n",
    "| `max_depth`               | Controls complexity (overfitting risk if too deep)                  |\n",
    "| `learning_rate`           | Smaller values = slower, more accurate learning                     |\n",
    "| `subsample`               | % of samples used per tree (helps generalization)                   |\n",
    "| `colsample_bytree`        | % of features used per tree                                         |\n",
    "| `gamma`                   | Minimum loss reduction to make a split (higher = more conservative) |\n",
    "| `reg_alpha`, `reg_lambda` | L1/L2 regularization (controls overfitting)                         |\n",
    "| `scale_pos_weight`        | Helps with imbalanced classes — gives class 1 more \"push\"           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c621e9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_parameter_tuning_XGB_randomSearch_recall(X_train, y_train):\n",
    "    xgb_param_grid = {\n",
    "        'clf__n_estimators': [100, 200, 300],\n",
    "        'clf__max_depth': [3, 5, 7, 10],\n",
    "        'clf__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'clf__subsample': [0.6, 0.8, 1.0],\n",
    "        'clf__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'clf__gamma': [0, 1, 5],\n",
    "        'clf__reg_alpha': [0, 0.1, 1],\n",
    "        'clf__reg_lambda': [0.5, 1, 5],\n",
    "        # 'clf__scale_pos_weight': [1, 2, 3]  # balances positive class (helpful for class 1 recall)\n",
    "        #clf__scale_pos_weight tidak dibutuhkan kalau data sudah balanced\n",
    "    }\n",
    "\n",
    "    xgb_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # optional, but safe to include\n",
    "        ('clf', XGBClassifier(eval_metric='logloss', use_label_encoder=False, random_state=42))\n",
    "    ])\n",
    "\n",
    "    recall_scorer = make_scorer(recall_score, pos_label=1)\n",
    "\n",
    "    xgb_random_search = RandomizedSearchCV(xgb_pipeline,\n",
    "                                    param_distributions=xgb_param_grid,\n",
    "                                    n_iter=50,\n",
    "                                    scoring=recall_scorer,\n",
    "                                    cv=5,\n",
    "                                    verbose=1,\n",
    "                                    n_jobs=-1,\n",
    "                                    random_state=42)\n",
    "    xgb_random_search.fit(X_train, y_train)\n",
    "    print(\"XGBoost ; Random Search \")\n",
    "    print(\"Best Parameters:\")\n",
    "    for key, value in  xgb_random_search.best_params_.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(\"--------------------------------------------\")    \n",
    "    \n",
    "    return xgb_random_search.best_estimator_ , xgb_random_search.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0743b364",
   "metadata": {},
   "source": [
    "##  hyper_parameter_tuning_XGB_randomSearch_accuracy(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a047f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hyper_parameter_tuning_XGB_randomSearch_accuracy(X_train, y_train):\n",
    "    xgb_param_grid = {\n",
    "        'clf__n_estimators': [100, 200, 300],\n",
    "        'clf__max_depth': [3, 5, 7, 10],\n",
    "        'clf__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'clf__subsample': [0.6, 0.8, 1.0],\n",
    "        'clf__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'clf__gamma': [0, 1, 5],\n",
    "        'clf__reg_alpha': [0, 0.1, 1],\n",
    "        'clf__reg_lambda': [0.5, 1, 5],\n",
    "        # 'clf__scale_pos_weight': [1, 2, 3] # #clf__scale_pos_weight tidak dibutuhkan kalau data sudah balanced\n",
    "    }\n",
    "\n",
    "    xgb_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', XGBClassifier(eval_metric='logloss', use_label_encoder=False, random_state=42))\n",
    "    ])\n",
    "\n",
    "    accuracy_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "    xgb_random_search = RandomizedSearchCV(\n",
    "        estimator=xgb_pipeline,\n",
    "        param_distributions=xgb_param_grid,\n",
    "        n_iter=50,\n",
    "        scoring=accuracy_scorer,\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    xgb_random_search.fit(X_train, y_train)\n",
    "\n",
    "    print(\"XGBoost ; Random Search (Accuracy Focused)\")\n",
    "    print(\"Best Parameters:\")\n",
    "    for key, value in xgb_random_search.best_params_.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(\"--------------------------------------------\")    \n",
    "\n",
    "    return xgb_random_search.best_estimator_, xgb_random_search.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55713ec",
   "metadata": {},
   "source": [
    "## hyper_parameter_tuning_XGB_randomSearch_AUC(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613fd2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_parameter_tuning_XGB_randomSearch_AUC(X_train, y_train):\n",
    "    xgb_param_grid = {\n",
    "        'clf__n_estimators': [100, 200, 300],\n",
    "        'clf__max_depth': [3, 5, 7, 10],\n",
    "        'clf__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'clf__subsample': [0.6, 0.8, 1.0],\n",
    "        'clf__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'clf__gamma': [0, 1, 5],\n",
    "        'clf__reg_alpha': [0, 0.1, 1],\n",
    "        'clf__reg_lambda': [0.5, 1, 5],\n",
    "        # 'clf__scale_pos_weight': [1, 2, 3]  # Uncomment if data imbalance is an issue\n",
    "    }\n",
    "\n",
    "    xgb_pipeline = Pipeline([\n",
    "        # ('scaler', StandardScaler()),  # ⛔ Commented: XGBoost usually handles feature scale well\n",
    "        ('clf', XGBClassifier(eval_metric='logloss', use_label_encoder=False, random_state=42))\n",
    "    ])\n",
    "\n",
    "    xgb_random_search = RandomizedSearchCV(\n",
    "        estimator=xgb_pipeline,\n",
    "        param_distributions=xgb_param_grid,\n",
    "        n_iter=50,\n",
    "        scoring='roc_auc',  # ✅ Ganti scoring ke AUC\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    xgb_random_search.fit(X_train, y_train)\n",
    "\n",
    "    print(\"XGBoost ; Random Search (AUC Focused)\")\n",
    "    print(\"Best Parameters:\")\n",
    "    for key, value in xgb_random_search.best_params_.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(\"--------------------------------------------\")    \n",
    "\n",
    "    return xgb_random_search.best_estimator_, xgb_random_search.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842cb016",
   "metadata": {},
   "source": [
    "## add_to_performance_report_comparison( model_name,y_test, y_pred, X_test,listBaru=False,scaler_name='', dataframe_name='',best_params='', tuning_focus='', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebd9ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_auc(model, X_test, y_test):\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8a3972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_performance_report_comparison( model_name,y_test, y_pred, X_test, listBaru=False,scaler_name='', dataframe_name='',best_params='', tuning_focus='', verbose=False):\n",
    "    try:\n",
    "        df_kompilasihasil=pd.read_excel(f'./output/dataframe_kompilasi_hasil_modelling.xlsx', index_col=None)\n",
    "        df_kompilasihasil = df_kompilasihasil.loc[:, ~df_kompilasihasil.columns.str.startswith('Unnamed')]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error reading the dataframe_kompilasi_hasil_modelling.xlsx :\", str(e))\n",
    "        df_kompilasihasil = None\n",
    "\n",
    "    model = load_model(f'./output/{model_name}') # untuk kebutuhan menghitung AUC\n",
    "\n",
    "    performance = {\n",
    "        'Model': [model_name],\n",
    "        'Accuracy': [accuracy_score(y_test, y_pred)],\n",
    "        'Precision': [precision_score(y_test, y_pred)],\n",
    "        'Recall': [recall_score(y_test, y_pred)],\n",
    "        'F1 Score': [f1_score(y_test, y_pred)],\n",
    "        'RoC-AuC': [calculate_auc(model, X_test, y_test)],\n",
    "        'scaler_name': [scaler_name],\n",
    "        'dataframe_name': [dataframe_name],\n",
    "        'tuning_focus': [tuning_focus],\n",
    "        'best_params': [str(best_params)]\n",
    "    }\n",
    "    tempDF = pd.DataFrame(performance)\n",
    "    if verbose :\n",
    "        print(tempDF)\n",
    "    \n",
    "    if listBaru == True:\n",
    "        result= tempDF\n",
    "    else:\n",
    "        if df_kompilasihasil is None:\n",
    "            result= tempDF\n",
    "        else:\n",
    "            result =pd.concat([df_kompilasihasil, tempDF], axis=0)\n",
    "\n",
    "    result = result.drop_duplicates()\n",
    "        \n",
    "    result.to_excel(f'./output/dataframe_kompilasi_hasil_modelling.xlsx', index=False)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8384b6",
   "metadata": {},
   "source": [
    "## plot_roc_curves_for_each_test(models_with_data: dict):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d0877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curves_for_each_test(models_with_data: dict,savedImageName):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    for name, (model, X_test, y_test) in models_with_data.items():\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            raise ValueError(f\"Model '{name}' does not support predict_proba().\")\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "        auc_score = roc_auc_score(y_test, y_proba)\n",
    "        plt.plot(fpr, tpr, label=f\"{name} (AUC = {auc_score:.3f})\")\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Random (AUC = 0.5)\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate (Recall)\")\n",
    "    plt.title(\"ROC Curve Comparison\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(savedImageName, dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c323667",
   "metadata": {},
   "source": [
    "# EDA , Data Cleansing and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee83744",
   "metadata": {},
   "source": [
    "## Create folder for output files\n",
    "\n",
    "buat tempat untuk menyimpan file output\n",
    "\n",
    "output files will be used for :\n",
    "\n",
    "- untuk double checking\n",
    "- for documentation \n",
    "- pickle files from model training result will be used for production\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9de34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = 'output'\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "    print(f\"Folder '{folder_name}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Folder '{folder_name}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426d8b9c",
   "metadata": {},
   "source": [
    "## Info Dasar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cef0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae9b957",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.replace(\"(\",\"_\",regex=False)\n",
    "df.columns = df.columns.str.replace(\")\",\"\",regex=False)\n",
    "df.columns = df.columns.str.replace(\" \",\"_\",regex=False)\n",
    "df.columns = df.columns.str.lower()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee115cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "daftarKolomOriginal = df.columns\n",
    "daftarKolomOriginal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97d1252",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = df.select_dtypes(include='number').columns.tolist()\n",
    "numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20222406",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_head = df.head()\n",
    "df_info = df.dtypes\n",
    "df_shape = df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184b31a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59952ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8404ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838808e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238e696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_duplicates = df.duplicated().sum()\n",
    "print(f\"Jumlah duplikasi: {num_duplicates}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f725a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplikacted_rows = df[df.duplicated(keep=False)].sort_values(list(df.columns))\n",
    "print(f\"duplikacted_rows:\\n{duplikacted_rows}\")\n",
    "duplikacted_rows.to_excel(r'.\\output\\xls_01_duplicated_rows.xlsx', index=False) # disimpan ke CSV for future checking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2238769",
   "metadata": {},
   "source": [
    "## remove duplcate Row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36e2bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shape = df.shape\n",
    "print(f\"Shape = {df_shape}\")\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "df_shape = df.shape\n",
    "print(f\"Shape after removing duplicates: {df_shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf9b2d7",
   "metadata": {},
   "source": [
    "removing duplicates 🟢\n",
    "checing for null values 🟢\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb107b0d",
   "metadata": {},
   "source": [
    "## check missing value \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20764208",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ff7609",
   "metadata": {},
   "source": [
    "## Mini EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef6a25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_eda(df,r'.\\output\\pdf_01_mini_eda_result.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162e348f",
   "metadata": {},
   "source": [
    "## backup original dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b5304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_000_original = df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd44d17",
   "metadata": {},
   "source": [
    "## capping / flagging by Impossible Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5577528",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_000_original.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7186e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_excel('./output/xls_02_smokerStatus_check_impossible_value_before.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7cebef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dd109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "impossible_thresholds = pd.DataFrame([\n",
    "    {'feature': 'age', 'min': 0, 'max': 120},\n",
    "    {'feature': 'height_cm', 'min': 120, 'max': 210},\n",
    "    {'feature': 'weight_kg', 'min': 35, 'max': 200},\n",
    "    {'feature': 'waist_cm', 'min': 50, 'max': 150},\n",
    "    {'feature': 'eyesight_left', 'min': 0.1, 'max': 2.0},\n",
    "    {'feature': 'eyesight_right', 'min': 0.1, 'max': 2.0},\n",
    "    {'feature': 'hearing_left', 'min': 0, 'max': 2},\n",
    "    {'feature': 'hearing_right', 'min': 0, 'max': 2},\n",
    "    {'feature': 'systolic', 'min': 90, 'max': 180},\n",
    "    {'feature': 'relaxation', 'min': 60, 'max': 120},\n",
    "    {'feature': 'fasting_blood_sugar', 'min': 70, 'max': 126},\n",
    "    {'feature': 'cholesterol', 'min': 125, 'max': 200},\n",
    "    {'feature': 'triglyceride', 'min': 50, 'max': 150},\n",
    "    {'feature': 'hdl', 'min': 40, 'max': 100},\n",
    "    {'feature': 'ldl', 'min': 50, 'max': 130},\n",
    "    {'feature': 'hemoglobin', 'min': 12.0, 'max': 17.5},\n",
    "    {'feature': 'urine_protein', 'min': 0, 'max': 4},\n",
    "    {'feature': 'serum_creatinine', 'min': 0.6, 'max': 1.3},\n",
    "    {'feature': 'ast', 'min': 8, 'max': 33},\n",
    "    {'feature': 'alt', 'min': 7, 'max': 56},\n",
    "    {'feature': 'gtp', 'min': 8, 'max': 61},\n",
    "    {'feature': 'dental_caries', 'min': 0, 'max': 1},\n",
    "])\n",
    "\n",
    "df = capping_features_based_on_threshold(df,impossible_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd27301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_excel('./output/xls_03_smokerStatus_check_impossible_value_after.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fb7777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini_eda(df,r'.\\output\\pdf_02_mini_eda_result_after_cap_threshold.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9f3495",
   "metadata": {},
   "source": [
    "## Check Target immbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab41a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['smoking'].value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23ffbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['smoking'].value_counts(normalize=True)\n",
    "#normalize = True (untuk tau distribusi dalam percent, bukan cuma jumlah count nya)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c10093d",
   "metadata": {},
   "source": [
    "## boxplots and distribution plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ba0f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for column in daftarKolomOriginal:\n",
    "    counter += 1  \n",
    "    \n",
    "    if column in df.columns:\n",
    "        lower_bound, upper_bound, num_outliers, percent_outliers, above_upper, below_lower = calculate_IQR(df, column)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(9, 3))\n",
    "        \n",
    "        sns.boxplot(data=df[column], ax=axes[0])\n",
    "        axes[0].set_title(f\"Boxplot of {column}\")\n",
    "        axes[0].axhline(y=lower_bound, color='r', linestyle='--', label=f'Lower Bound: {lower_bound}')\n",
    "        axes[0].axhline(y=upper_bound, color='g', linestyle='--', label=f'Upper Bound: {upper_bound}')\n",
    "        \n",
    "        text_str = f\"Lower Bound: {lower_bound}\\nUpper Bound: {upper_bound}\\n\" \\\n",
    "                   f\"Outliers: {num_outliers}\\n% Outliers: {percent_outliers:.2f}%\\n\" \\\n",
    "                   f\"Above Upper: {above_upper}\\nBelow Lower: {below_lower}\"\n",
    "        \n",
    "        axes[0].text(1.5, 0.5, text_str, transform=axes[0].transAxes,\n",
    "                     fontsize=10, verticalalignment='center', horizontalalignment='left',\n",
    "                     bbox=dict(facecolor='white', alpha=0.7, edgecolor='black', boxstyle='round,pad=1'))\n",
    "        \n",
    "        sns.histplot(df[column], kde=True, bins=50, ax=axes[1])\n",
    "        axes[1].set_title(f\"Distribution of {column}\")\n",
    "        axes[1].set_xlabel(column)\n",
    "        axes[1].set_ylabel(\"Count\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.savefig(f'./output/plot_01_boxplot_and_distributionplot_{str(counter).zfill(2)}_of_{column}.jpg', dpi=300)\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(f\"Column '{column}' not found in the DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2112be0f",
   "metadata": {},
   "source": [
    "## Check for NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235da7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8f855b",
   "metadata": {},
   "source": [
    "# Balancing before feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c41b88",
   "metadata": {},
   "source": [
    "## add combined feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d35bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_M2_step01 = df_000_original.copy(deep=True)\n",
    "df_M2_step01 = add_combined_features(df_M2_step01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6690eeb9",
   "metadata": {},
   "source": [
    "## Splitting_balancing_scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9e08de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train, X_test, y_test,M2_scalernya = splitting_balancing_scaling(df_M2_step01, 'smoking',df_name ='df_M2_step01' )\n",
    "\n",
    "save_model(M2_scalernya,f'./output/model_M2_scalernya.pkl')\n",
    "\n",
    "\n",
    "X_train_df =  pd.DataFrame(X_train)\n",
    "X_train_df.columns = df_M2_step01.drop(['smoking'], axis=1).columns\n",
    "X_train_df.reset_index(drop=True, inplace=True) \n",
    "y_train_df = pd.Series(y_train, name='smoking') \n",
    "y_train_df.reset_index(drop=True, inplace=True) \n",
    "df_M2_step02_train = pd.concat([X_train_df, y_train_df], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "X_test_df =  pd.DataFrame(X_test)\n",
    "X_test_df.columns = df_M2_step01.drop(['smoking'], axis=1).columns\n",
    "X_test_df.reset_index(drop=True, inplace=True) \n",
    "y_test_df = pd.Series(y_test, name='smoking') \n",
    "y_test_df.reset_index(drop=True, inplace=True)\n",
    "df_M2_step02_test = pd.concat([X_test_df, y_test_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70e3157",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_M2_step01.shape)\n",
    "print(df_M2_step02_train.shape)\n",
    "print(df_M2_step02_test.shape)\n",
    "nan_rows = df_M2_step02_test[df_M2_step02_test.isna().any(axis=1)]\n",
    "print(nan_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219bbd63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97653ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_M2_step02_train_corr = df_M2_step02_train.corr(numeric_only=True)\n",
    "show_corrMap(df_M2_step02_train_corr,'./output/plot_04_correlationMap_df_M2_step02_train_corr.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74b945d",
   "metadata": {},
   "source": [
    "## Check whether new feature is good or bad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16dfdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# menggunakan fungsi : def drop_new_feature_yg_lemah_atau_feature_originalnya(df,listnya):\n",
    "    # fungsi ini membandingkan 2 feature pembentuk feature baru \n",
    "    # listnya adalah list dengan urutan FeatureLama1, FeatureLama2, FeatureBentukanBaru\n",
    "    # jika feature baru justru lebih lemah maka feature baru dibuang saja (gak jadi dipakai)\n",
    "    # jika feature baru paling tinggi , maka feature lama keduanya dibuang\n",
    "    # jika salah satu feature saja yang lebih lemah dari feature baru, maka feature lemah itu yang dibuang\n",
    "    \n",
    "\n",
    "df_M2_step02_train=drop_new_feature_yg_lemah_atau_feature_originalnya(df_M2_step02_train,['weight_kg', 'height_cm', 'new_bmi'])\n",
    "\n",
    "df_M2_step02_train=drop_new_feature_yg_lemah_atau_feature_originalnya(df_M2_step02_train,['waist_cm', 'height_cm', 'new_waist_height_ratio'])\n",
    "\n",
    "df_M2_step02_train=drop_new_feature_yg_lemah_atau_feature_originalnya(df_M2_step02_train,['systolic', 'relaxation', 'new_pulse_preasure'])\n",
    "\n",
    "df_M2_step02_train=drop_new_feature_yg_lemah_atau_feature_originalnya(df_M2_step02_train,['eyesight_right', 'eyesight_left', 'new_vision'])\n",
    "\n",
    "df_M2_step02_train=drop_new_feature_yg_lemah_atau_feature_originalnya(df_M2_step02_train,['hearing_right', 'hearing_left', 'new_hearing'])\n",
    "\n",
    "df_M2_step02_train=drop_new_feature_yg_lemah_atau_feature_originalnya(df_M2_step02_train,['hdl', 'cholesterol', 'new_good_chol_ratio'])\n",
    "\n",
    "df_M2_step02_train=drop_new_feature_yg_lemah_atau_feature_originalnya(df_M2_step02_train,['ldl', 'hdl', 'new_bad_good_chol_ratio'])\n",
    "\n",
    "df_M2_step02_train=drop_new_feature_yg_lemah_atau_feature_originalnya(df_M2_step02_train,['ast', 'alt', 'new_liverEnzimeRatio'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ca9705",
   "metadata": {},
   "source": [
    "## Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d195676",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_M2_step02_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e92cf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_M2_step02_train_corr = df_M2_step02_train.corr(numeric_only=True)\n",
    "show_corrMap(df_M2_step02_train_corr,'./output/plot_04_correlationMap_df_M2_step02_train_corr.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e303ccf4",
   "metadata": {},
   "source": [
    "### drop feature yang correlation lemah < 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024086f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_threshold = 0.05 # correlation threshold\n",
    "selected_features_prep=df_M2_step02_train_corr['smoking']\n",
    "print(f'sebelum dipilih=\\n{selected_features_prep}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9ab146",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = selected_features_prep[abs(selected_features_prep) > selection_threshold]\n",
    "print('')\n",
    "print(f'selected_featres=\\n{selected_features}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989f7acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature_names = selected_features.index.to_list()\n",
    "selected_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d9f1e2",
   "metadata": {},
   "source": [
    "### remove feature with high Multicoliniearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75909048",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_M2_step03_train = df_M2_step02_train[selected_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea6bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_result = calculate_vif(df_M2_step03_train,'smoking', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dcc1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_M2_step03_train.drop(['new_good_chol_ratio'], axis=1, inplace=True)\n",
    "vif_result = calculate_vif(df_M2_step03_train,'smoking', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93006f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_M2_step03_train.drop(['weight_kg'], axis=1, inplace=True)\n",
    "vif_result = calculate_vif(df_M2_step03_train,'smoking', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3051126a",
   "metadata": {},
   "source": [
    "## remove features with high correlation betwen features (>0.75)\n",
    "\n",
    "remove the one that has weaker correlation with target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1050c849",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'smoking' \n",
    "df_M2_step04_train, removed_features = remove_highly_correlated_features(df_M2_step03_train, target_column, threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcd192e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60d150a1",
   "metadata": {},
   "source": [
    "# Baseline Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b29e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMENT SALAH SATU BLOK DI BAWAHA INI UNTUK MEMILIH DATA DARI DATA FRAME MANA YANG MAU PAKAI UNTUK MODELLING\n",
    "\n",
    "#=====================================================\n",
    "# the_df = df_000_original.select_dtypes(include='number').copy(deep=True)\n",
    "# nama_dataframe_asal= 'df_000_original_155rb'\n",
    "# X_train,y_train, X_test, y_test,scalernya = splitting_balancing_scaling(the_df, target_column, nama_dataframe_asal)\n",
    "# X_train,y_train, X_test, y_test,scalernya = splitting_balancing_scaling(the_df, target_column, nama_dataframe_asal)\n",
    "# scaler_name = f'./output/model_{nama_dataframe_asal}_scalernya.pkl'\n",
    "# save_model(scalernya, f'./output/model_{nama_dataframe_asal}_scalernya.pkl')\n",
    "\n",
    "\n",
    "# #=====================================================\n",
    "the_df = df_M2_step04_train.copy(deep=True)\n",
    "nama_dataframe_asal= 'df_M2_step04_train'\n",
    "selected_column =list(df_M2_step04_train.columns)\n",
    "for col in df_M2_step02_test.columns:\n",
    "    if col not in df_M2_step04_train:\n",
    "        df_M2_step02_test.drop(columns=col, inplace=True)\n",
    "X_train = df_M2_step04_train.drop(columns='smoking')\n",
    "y_train = df_M2_step04_train['smoking']\n",
    "X_test = df_M2_step02_test.drop(columns='smoking')\n",
    "y_test = df_M2_step02_test['smoking']\n",
    "scaler_name = 'model_M2_scalernya.pkl'\n",
    "# ======================================================================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5400da5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'smoking'\n",
    "\n",
    "print(f'===================================================')\n",
    "print(\".\")\n",
    "print(\".\")\n",
    "\n",
    "# Logistic Regression\n",
    "modelnya = LogisticRegression()\n",
    "modelnya.fit(X_train, y_train)\n",
    "model_name = f'model_{nama_dataframe_asal}_logreg_basic.pkl'\n",
    "model_file_full_path = f'./output/{model_name}'\n",
    "save_model(modelnya, model_file_full_path)\n",
    "model = load_model(model_file_full_path)\n",
    "y_pred = model.predict(X_test)\n",
    "# print(f\"== {model_name} ==\")\n",
    "# print(classification_report(y_test, y_pred))\n",
    "df_kompilasihasil=add_to_performance_report_comparison( model_name,y_test, y_pred,X_test,False,scaler_name,nama_dataframe_asal)\n",
    "\n",
    "model_logregr=modelnya\n",
    "\n",
    "# Random Forest Classifier\n",
    "modelnya = RandomForestClassifier()\n",
    "modelnya.fit(X_train, y_train)\n",
    "model_name = f'model_{nama_dataframe_asal}_randomForest_basic.pkl'\n",
    "model_file_full_path = f'./output/{model_name}'\n",
    "save_model(modelnya, model_file_full_path)\n",
    "model = load_model(model_file_full_path)\n",
    "y_pred = model.predict(X_test)\n",
    "# print(f\"== {model_name} ==\")\n",
    "# print(classification_report(y_test, y_pred))\n",
    "df_kompilasihasil=add_to_performance_report_comparison( model_name,y_test, y_pred,X_test,False,scaler_name,nama_dataframe_asal)\n",
    "\n",
    "model_rndfore=modelnya\n",
    "\n",
    "\n",
    "\n",
    "# XGBoost Classifier\n",
    "modelnya = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "modelnya.fit(X_train, y_train)\n",
    "model_name = f'model_{nama_dataframe_asal}_xgbost_basic.pkl'\n",
    "model_file_full_path = f'./output/{model_name}'\n",
    "save_model(modelnya, model_file_full_path)\n",
    "model = load_model(model_file_full_path)\n",
    "y_pred = model.predict(X_test)\n",
    "# print(f\"== {model_name} ==\")\n",
    "# print(classification_report(y_test, y_pred))\n",
    "df_kompilasihasil=add_to_performance_report_comparison( model_name,y_test, y_pred,X_test,False,scaler_name,nama_dataframe_asal)\n",
    "\n",
    "model_xgboost=modelnya\n",
    "\n",
    "\n",
    "\n",
    "models_with_data = {\n",
    "    \"LogRegr\": (model_logregr, X_test, y_test),\n",
    "    \"RndFore\": (model_rndfore, X_test, y_test),\n",
    "    \"XGBoost\": (model_xgboost, X_test, y_test),\n",
    "}\n",
    "plot_roc_curves_for_each_test(models_with_data,f'./output/plot_10_ROC_Curve_model_df_M2_step04_train_gabungan_basic.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b87e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kompilasihasil.sort_values(by='Recall', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608e8e1e",
   "metadata": {},
   "source": [
    "| **Metric**              | **Why it matters**                                                                                                                                                                                |\n",
    "| ----------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Recall (class 1)**    | Measures how many real smokers the model correctly detects. <br>Use when it's critical not to miss any smokers. Example: in medical screening, you want to identify everyone who needs attention. |\n",
    "| **F1-score (class 1)**  | Combines both precision and recall. <br>Use this when you want a **balanced performance** — not too many false negatives, and not too many false positives.                                       |\n",
    "| **Precision (class 1)** | Measures how many of the predicted smokers are actually smokers. <br>Use this when **predicting someone as a smoker has serious consequences** (e.g., legal, privacy, or social impact).          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e49fc0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edaf059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b808bd7",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning (Focus on Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e21ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMENT SALAH SATU BLOK DI BAWAHA INI UNTUK MEMILIH DATA DARI DATA FRAME MANA YANG MAU PAKAI UNTUK MODELLING\n",
    "\n",
    "#=====================================================\n",
    "# the_df = df_000_original.select_dtypes(include='number').copy(deep=True)\n",
    "# nama_dataframe_asal= 'df_000_original_155rb'\n",
    "# X_train,y_train, X_test, y_test,scalernya = splitting_balancing_scaling(the_df, target_column, nama_dataframe_asal)\n",
    "# X_train,y_train, X_test, y_test,scalernya = splitting_balancing_scaling(the_df, target_column, nama_dataframe_asal)\n",
    "# scaler_name = f'./output/model_{nama_dataframe_asal}_scalernya.pkl'\n",
    "# save_model(scalernya, f'./output/model_{nama_dataframe_asal}_scalernya.pkl')\n",
    "\n",
    "the_df = df_M2_step04_train.copy(deep=True)\n",
    "nama_dataframe_asal= 'df_M2_step04_train'\n",
    "selected_column =list(df_M2_step04_train.columns)\n",
    "for col in df_M2_step02_test.columns:\n",
    "    if col not in df_M2_step04_train:\n",
    "        df_M2_step02_test.drop(columns=col, inplace=True)\n",
    "X_train = df_M2_step04_train.drop(columns='smoking')\n",
    "y_train = df_M2_step04_train['smoking']\n",
    "X_test = df_M2_step02_test.drop(columns='smoking')\n",
    "y_test = df_M2_step02_test['smoking']\n",
    "scaler_name = 'model_M2_scalernya.pkl'\n",
    "# ======================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34afe8b",
   "metadata": {},
   "source": [
    "## GridSearch for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85697ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model,best_model_params = hyper_parameter_tuning_LogisticReg_gridSearch_recall(X_train, y_train)\n",
    "# best_model.fit(X_train, y_train)\n",
    "\n",
    "model_name = f'model_{nama_dataframe_asal}_logreg_bestModel_recall.pkl'\n",
    "model_file_full_path = f'./output/{model_name}'\n",
    "\n",
    "save_model(best_model,model_file_full_path)\n",
    "\n",
    "\n",
    "model = load_model(model_file_full_path)\n",
    "y_pred = model.predict(X_test)\n",
    "df_kompilasihasil=add_to_performance_report_comparison( model_name,y_test, y_pred,X_test,False,scaler_name,nama_dataframe_asal,best_model_params,'Tuned for Recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760b5d6c",
   "metadata": {},
   "source": [
    "## Random Search for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662fdb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model,best_model_params = hyper_parameter_tuning_randomForest_randomSearch_recall(X_train, y_train)\n",
    "# best_model.fit(X_train, y_train)\n",
    "\n",
    "model_name = f'model_{nama_dataframe_asal}_randomForest_bestModel_recall.pkl'\n",
    "model_file_full_path = f'./output/{model_name}'\n",
    "\n",
    "save_model(best_model,model_file_full_path)\n",
    "\n",
    "model = load_model(model_file_full_path)\n",
    "y_pred = model.predict(X_test)\n",
    "df_kompilasihasil=add_to_performance_report_comparison( model_name,y_test, y_pred,X_test,False,scaler_name,nama_dataframe_asal,best_model_params,'Tuned for Recall')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bba8828",
   "metadata": {},
   "source": [
    "## RandomSearch for XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212f353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model,best_model_params = hyper_parameter_tuning_XGB_randomSearch_recall(X_train, y_train)\n",
    "# best_model.fit(X_train, y_train)\n",
    "\n",
    "model_name = f'model_{nama_dataframe_asal}_xgboost_bestModel_recall.pkl'\n",
    "model_file_full_path = f'./output/{model_name}'\n",
    "\n",
    "save_model(best_model,model_file_full_path)\n",
    "\n",
    "model = load_model(model_file_full_path)\n",
    "y_pred = model.predict(X_test)\n",
    "df_kompilasihasil=add_to_performance_report_comparison( model_name,y_test, y_pred,X_test,False,scaler_name,nama_dataframe_asal,best_model_params,'Tuned for Recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c882065",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning (Focuks on Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f0db12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMENT SALAH SATU BLOK DI BAWAHA INI UNTUK MEMILIH DATA DARI DATA FRAME MANA YANG MAU PAKAI UNTUK MODELLING\n",
    "\n",
    "#=====================================================\n",
    "# the_df = df_000_original.select_dtypes(include='number').copy(deep=True)\n",
    "# nama_dataframe_asal= 'df_000_original_155rb'\n",
    "# X_train,y_train, X_test, y_test,scalernya = splitting_balancing_scaling(the_df, target_column, nama_dataframe_asal)\n",
    "# X_train,y_train, X_test, y_test,scalernya = splitting_balancing_scaling(the_df, target_column, nama_dataframe_asal)\n",
    "# scaler_name = f'./output/model_{nama_dataframe_asal}_scalernya.pkl'\n",
    "# save_model(scalernya, f'./output/model_{nama_dataframe_asal}_scalernya.pkl')\n",
    "\n",
    "the_df = df_M2_step04_train.copy(deep=True)\n",
    "nama_dataframe_asal= 'df_M2_step04_train'\n",
    "selected_column =list(df_M2_step04_train.columns)\n",
    "for col in df_M2_step02_test.columns:\n",
    "    if col not in df_M2_step04_train:\n",
    "        df_M2_step02_test.drop(columns=col, inplace=True)\n",
    "X_train = df_M2_step04_train.drop(columns='smoking')\n",
    "y_train = df_M2_step04_train['smoking']\n",
    "X_test = df_M2_step02_test.drop(columns='smoking')\n",
    "y_test = df_M2_step02_test['smoking']\n",
    "scaler_name = 'model_M2_scalernya.pkl'\n",
    "# ======================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08fbdb9",
   "metadata": {},
   "source": [
    "## GridSearch for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f055e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model,best_model_params = hyper_parameter_tuning_LogisticReg_gridSearch_accuracy(X_train, y_train)\n",
    "# best_model.fit(X_train, y_train)\n",
    "\n",
    "model_name = f'model_{nama_dataframe_asal}_logreg_bestModel_Accuracy.pkl'\n",
    "model_file_full_path = f'./output/{model_name}'\n",
    "\n",
    "save_model(best_model,model_file_full_path)\n",
    "\n",
    "model = load_model(model_file_full_path)\n",
    "y_pred = model.predict(X_test)\n",
    "df_kompilasihasil=add_to_performance_report_comparison( model_name,y_test, y_pred,X_test,False,scaler_name,nama_dataframe_asal,best_model_params,'Tuned for Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f047d440",
   "metadata": {},
   "source": [
    "## RandomSearch for RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a21c360",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model,best_model_params = hyper_parameter_tuning_randomForest_randomSearch_accuracy(X_train, y_train)\n",
    "# best_model.fit(X_train, y_train)\n",
    "\n",
    "model_name = f'model_{nama_dataframe_asal}_randomForest_bestModel_Accuracy.pkl'\n",
    "model_file_full_path = f'./output/{model_name}'\n",
    "\n",
    "save_model(best_model,model_file_full_path)\n",
    "\n",
    "\n",
    "model = load_model(model_file_full_path)\n",
    "y_pred = model.predict(X_test)\n",
    "df_kompilasihasil=add_to_performance_report_comparison( model_name,y_test, y_pred,X_test,False,scaler_name,nama_dataframe_asal,best_model_params,'Tuned for Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160f25af",
   "metadata": {},
   "source": [
    "## RandomSearch for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1608ebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model,best_model_params = hyper_parameter_tuning_XGB_randomSearch_accuracy(X_train, y_train)\n",
    "# best_model.fit(X_train, y_train)\n",
    "\n",
    "model_name = f'model_{nama_dataframe_asal}_xgboost_bestModel_Accuracy.pkl'\n",
    "model_file_full_path = f'./output/{model_name}'\n",
    "\n",
    "save_model(best_model,model_file_full_path)\n",
    "\n",
    "model = load_model(model_file_full_path)\n",
    "y_pred = model.predict(X_test)\n",
    "df_kompilasihasil=add_to_performance_report_comparison( model_name,y_test, y_pred,X_test,False,scaler_name,nama_dataframe_asal,best_model_params,'Tuned for Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca13739a",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning (Focus on AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a92a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMENT SALAH SATU BLOK DI BAWAHA INI UNTUK MEMILIH DATA DARI DATA FRAME MANA YANG MAU PAKAI UNTUK MODELLING\n",
    "\n",
    "#=====================================================\n",
    "# the_df = df_000_original.select_dtypes(include='number').copy(deep=True)\n",
    "# nama_dataframe_asal= 'df_000_original_155rb'\n",
    "# X_train,y_train, X_test, y_test,scalernya = splitting_balancing_scaling(the_df, target_column, nama_dataframe_asal)\n",
    "# X_train,y_train, X_test, y_test,scalernya = splitting_balancing_scaling(the_df, target_column, nama_dataframe_asal)\n",
    "# scaler_name = f'./output/model_{nama_dataframe_asal}_scalernya.pkl'\n",
    "# save_model(scalernya, f'./output/model_{nama_dataframe_asal}_scalernya.pkl')\n",
    "\n",
    "\n",
    "the_df = df_M2_step04_train.copy(deep=True)\n",
    "nama_dataframe_asal= 'df_M2_step04_train'\n",
    "selected_column =list(df_M2_step04_train.columns)\n",
    "for col in df_M2_step02_test.columns:\n",
    "    if col not in df_M2_step04_train:\n",
    "        df_M2_step02_test.drop(columns=col, inplace=True)\n",
    "X_train = df_M2_step04_train.drop(columns='smoking')\n",
    "y_train = df_M2_step04_train['smoking']\n",
    "X_test = df_M2_step02_test.drop(columns='smoking')\n",
    "y_test = df_M2_step02_test['smoking']\n",
    "scaler_name = 'model_M2_scalernya.pkl'\n",
    "# ======================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8969019d",
   "metadata": {},
   "source": [
    "## GridSearch for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff487b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model,best_model_params = hyper_parameter_tuning_LogisticReg_gridSearch_AUC(X_train, y_train)\n",
    "# best_model.fit(X_train, y_train)\n",
    "\n",
    "model_name = f'model_{nama_dataframe_asal}_logreg_bestModel_AUC.pkl'\n",
    "model_file_full_path = f'./output/{model_name}'\n",
    "\n",
    "save_model(best_model,model_file_full_path)\n",
    "\n",
    "model = load_model(model_file_full_path)\n",
    "y_pred = model.predict(X_test)\n",
    "df_kompilasihasil=add_to_performance_report_comparison( model_name,y_test, y_pred,X_test,False,scaler_name,nama_dataframe_asal,best_model_params,'Tuned for AUC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6d8289",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_with_data = {\n",
    "    \"LogReg\": (model, X_test, y_test),\n",
    "}\n",
    "plot_roc_curves_for_each_test(models_with_data,f'./output/plot_10_ROC_AUC_Curve_{model_name}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2975a38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logreg=model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91dea16",
   "metadata": {},
   "source": [
    "## RandomSearch for RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011fafd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model,best_model_params = hyper_parameter_tuning_randomForest_randomSearch_AUC(X_train, y_train)\n",
    "# best_model.fit(X_train, y_train)\n",
    "\n",
    "model_name = f'model_{nama_dataframe_asal}_randomForest_bestModel_AUC.pkl'\n",
    "model_file_full_path = f'./output/{model_name}'\n",
    "\n",
    "save_model(best_model,model_file_full_path)\n",
    "\n",
    "\n",
    "model = load_model(model_file_full_path)\n",
    "y_pred = model.predict(X_test)\n",
    "df_kompilasihasil=add_to_performance_report_comparison( model_name,y_test, y_pred,X_test,False,scaler_name,nama_dataframe_asal,best_model_params,'Tuned for AUC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f5d0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_with_data = {\n",
    "    \"RandomForest\": (model, X_test, y_test),\n",
    "}\n",
    "plot_roc_curves_for_each_test(models_with_data,f'./output/plot_10_ROC_AUC_Curve_{model_name}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ede410",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf=model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee608be",
   "metadata": {},
   "source": [
    "## RandomSearch for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e75cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model,best_model_params = hyper_parameter_tuning_XGB_randomSearch_AUC(X_train, y_train)\n",
    "# best_model.fit(X_train, y_train)\n",
    "\n",
    "model_name = f'model_{nama_dataframe_asal}_xgboost_bestModel_AUC.pkl'\n",
    "model_file_full_path = f'./output/{model_name}'\n",
    "\n",
    "save_model(best_model,model_file_full_path)\n",
    "\n",
    "model = load_model(model_file_full_path)\n",
    "y_pred = model.predict(X_test)\n",
    "df_kompilasihasil=add_to_performance_report_comparison( model_name,y_test, y_pred,X_test,False,scaler_name,nama_dataframe_asal,best_model_params,'Tuned for AUC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234c19ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_with_data = {\n",
    "    \"XGBoost\": (model, X_test, y_test),\n",
    "}\n",
    "plot_roc_curves_for_each_test(models_with_data,f'./output/plot_10_ROC_AUC_Curve_{model_name}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1322a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f625d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_with_data = {\n",
    "    \"LogisticReg\": (model_logreg, X_test, y_test),\n",
    "    \"RandomForest\": (model_rf, X_test, y_test),\n",
    "    \"XGBoost\": (model_xgb, X_test, y_test)\n",
    "}\n",
    "plot_roc_curves_for_each_test(models_with_data,f'./output/plot_10_ROC_AUC_Curve_Gabungan_AUC.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eca8982",
   "metadata": {},
   "source": [
    "# Selecting Best Modelling for Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d7aab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kompilasihasil_sorted = df_kompilasihasil.sort_values(by='RoC-AuC', ascending=False)\n",
    "df_kompilasihasil_sorted\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datly)",
   "language": "python",
   "name": "datly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
